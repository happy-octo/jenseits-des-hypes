{{- if .Values.openshiftAi.installed }}
{{- if not (lookup "template.openshift.io/v1" "Template" "" "ollama-serving-runtime") }}
kind: Template
apiVersion: template.openshift.io/v1
metadata:
  name: ollama-serving-runtime
  namespace: redhat-ods-applications
  annotations:
    description: Ollama allows you to run large language models, such as Llama 2 and Code Llama, without any registration or waiting list
    internal.config.kubernetes.io/previousKinds: Template
    internal.config.kubernetes.io/previousNames: vllm-runtime-template
    internal.config.kubernetes.io/previousNamespaces: opendatahub
    opendatahub.io/apiProtocol: REST
    opendatahub.io/modelServingSupport: '["single"]'
    openshift.io/display-name: Ollama ServingRuntime for KServe
    openshift.io/provider-display-name: Red Hat, Inc.
    tags: rhods,rhoai,kserve,servingruntime
    template.openshift.io/long-description: This template defines resources needed to deploy vLLM ServingRuntime with KServe in Red Hat OpenShift AI
  labels:
    app: odh-dashboard
    app.kubernetes.io/part-of: kserve
    app.opendatahub.io/kserve: "true"
    opendatahub.io/dashboard: "true"
    opendatahub.io/ootb: "true"
objects:
  - apiVersion: serving.kserve.io/v1alpha1
    kind: ServingRuntime
    labels:
      name: ollama
    metadata:
      annotations:
        opendatahub.io/recommended-accelerators: ''
        openshift.io/display-name: Ollama
      name: ollama
    spec:
      annotations:
        prometheus.io/path: /metrics # ?
        prometheus.io/port: "11434" # ?
      builtInAdapter:
        modelLoadingTimeoutMillis: 90000
      containers:
        - image: quay.io/rh-aiservices-bu/ollama-ubi9:0.2.8
          env:
            - name: OLLAMA_MODELS
              value: /.ollama/models
            - name: OLLAMA_HOST
              value: 0.0.0.0
            - name: OLLAMA_KEEP_ALIVE
              value: -1m
          name: kserve-container
          ports:
            - containerPort: 11434
              name: http1
              protocol: TCP
      multiModel: false
      supportedModelFormats:
        - autoSelect: true
          name: Ollama
{{- end }}
---
apiVersion: serving.kserve.io/v1alpha1
kind: ServingRuntime
metadata:
  annotations:
    opendatahub.io/accelerator-name: ''
    opendatahub.io/apiProtocol: REST
    opendatahub.io/recommended-accelerators: ''
    opendatahub.io/template-display-name: Ollama
    opendatahub.io/template-name: ollama
    openshift.io/display-name: {{ .Release.Name }}
  name: {{ .Release.Name }}
  namespace: {{ .Release.Namespace }}
  labels:
    opendatahub.io/dashboard: 'true'
spec:
  annotations:
    prometheus.io/path: /metrics
    prometheus.io/port: '11434'
  builtInAdapter:
    modelLoadingTimeoutMillis: 90000
  containers:
    - env:
        - name: OLLAMA_MODELS
          value: /.ollama/models
        - name: OLLAMA_HOST
          value: 0.0.0.0
        - name: OLLAMA_KEEP_ALIVE
          value: '-1m'
      image: 'quay.io/rh-aiservices-bu/ollama-ubi9:0.2.8'
      name: kserve-container
      ports:
        - containerPort: 11434
          name: http1
          protocol: TCP
      volumeMounts:
        - mountPath: /dev/shm
          name: shm
  multiModel: false
  supportedModelFormats:
    - autoSelect: true
      name: Ollama
  volumes:
    - emptyDir:
        medium: Memory
        sizeLimit: 2Gi
      name: shm
---
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  annotations:
    openshift.io/display-name: {{ .Release.Name }}
    serving.knative.openshift.io/enablePassthrough: 'true'
    serving.kserve.io/deploymentMode: RawDeployment
    sidecar.istio.io/inject: 'true'
    sidecar.istio.io/rewriteAppHTTPProbers: 'true'
  name: {{ .Release.Name }}
  namespace: {{ .Release.Namespace }}
  labels:
    networking.knative.dev/visibility: cluster-local
    opendatahub.io/dashboard: 'true'
spec:
  predictor:
    maxReplicas: 1
    minReplicas: 1
    model:
      modelFormat:
        name: Ollama
      name: ''
      resources:
        limits:
          cpu: '2'
          memory: 8Gi
        requests:
          cpu: '1'
          memory: 4Gi
      runtime: {{ .Release.Name }}
      storage:
        key: aws-connection-{{ .Release.Name }}
        path: ollama
{{- end }}
